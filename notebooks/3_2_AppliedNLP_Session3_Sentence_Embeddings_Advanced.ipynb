{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa65d751",
   "metadata": {},
   "source": [
    "# Session 3 — Sentence-Level Analysis\n",
    "## Measure 3: Sentence Embeddings (LLMs as Semantic Encoders)\n",
    "### BONUS: Comparative Analysis of PetSemetary and TheShining\n",
    "\n",
    "> **Note:** This is an **advanced extension** of notebook `3_1_AppliedNLP_Session3_Sentence_Embeddings.ipynb`. \n",
    "> Complete the basic notebook first to understand the fundamentals of sentence embeddings before working through this more complex literary analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### What This Bonus Notebook Covers\n",
    "This advanced demonstration applies sentence embeddings to **real literary texts**, analyzing Stephen King's two PetSemetary books:\n",
    "- Extracts and compares key sentences from both novels\n",
    "- Performs multi-dimensional visualization (2D, 3D)\n",
    "- Automatically discovers thematic clusters\n",
    "- Quantifies cross-book semantic similarities\n",
    "- Provides comparative literary analysis\n",
    "\n",
    "### Prerequisites\n",
    "Before starting this notebook, you should:\n",
    "- Complete notebook `3_1` and understand basic sentence embeddings\n",
    "- Be familiar with PCA (Principal Component Analysis)\n",
    "- Understand cosine similarity metrics\n",
    "- Have the PetSemetary books in the `../data/` folder\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Review: What are Sentence Embeddings?\n",
    "Sentence embeddings are **dense vector representations** of sentences that capture their semantic meaning in a high-dimensional space. Unlike traditional word-based approaches, sentence embeddings encode the entire meaning of a sentence into a single fixed-length vector (typically 384-768 dimensions).\n",
    "\n",
    "### How They Work\n",
    "Modern sentence embedding models (like SBERT - Sentence-BERT) use **transformer architectures** pre-trained on massive text corpora. These models:\n",
    "- Convert sentences into numerical vectors where semantically similar sentences are close together in vector space\n",
    "- Capture context, syntax, and semantic relationships\n",
    "- Enable comparison of sentences based on meaning rather than just word overlap\n",
    "\n",
    "### Key Applications in NLP\n",
    "1. **Semantic Similarity**: Measure how similar two sentences are in meaning\n",
    "2. **Information Retrieval**: Find relevant documents or passages based on semantic search\n",
    "3. **Text Clustering**: Group similar sentences or documents together\n",
    "4. **Duplicate Detection**: Identify paraphrases or semantically identical content\n",
    "5. **Question Answering**: Match questions with relevant answers\n",
    "6. **Content Recommendation**: Suggest similar content based on semantic understanding\n",
    "7. **Comparative Literary Analysis**: Compare themes, writing styles, and narrative patterns across texts\n",
    "\n",
    "### This Advanced Demonstration\n",
    "We'll use the **all-MiniLM-L6-v2** model to:\n",
    "- Extract key sentences from both PetSemetary books using extractive summarization\n",
    "- Encode sentences into semantic vectors\n",
    "- Visualize thematic relationships between the two books in 2D/3D space\n",
    "- Identify semantic similarities and differences between the narratives\n",
    "- Cluster sentences by themes (e.g., size changes, absurdity, dialogue, fantasy)\n",
    "- Perform quantitative comparative literature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e63d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NPL\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "465bbe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PetSemetary: 812,517 characters\n",
      "TheShining: 906,176 characters\n"
     ]
    }
   ],
   "source": [
    "# Load the two PetSemetarymetary books\n",
    "def load_book(filepath):\n",
    "    \"\"\"Load and clean book text\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Remove Project Gutenberg headers/footers\n",
    "    start_markers = ['*** START OF', 'CHAPTER I']\n",
    "    end_markers = ['*** END OF', 'End of Project Gutenberg']\n",
    "    \n",
    "    # Find start\n",
    "    for marker in start_markers:\n",
    "        if marker in text:\n",
    "            start = text.find(marker)\n",
    "            if marker == 'CHAPTER I':\n",
    "                text = text[start:]\n",
    "            else:\n",
    "                text = text[start + len(marker):]\n",
    "            break\n",
    "    \n",
    "    # Find end\n",
    "    for marker in end_markers:\n",
    "        if marker in text:\n",
    "            end = text.find(marker)\n",
    "            text = text[:end]\n",
    "            break\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Load both books\n",
    "PetSemetary_text = load_book('../data/PetSemetary.txt')\n",
    "TheShining_text = load_book('../data/TheShining.txt')\n",
    "\n",
    "print(f\"PetSemetary: {len(PetSemetary_text):,} characters\")\n",
    "print(f\"TheShining: {len(TheShining_text):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ebf308",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\yassi/nltk_data'\n    - 'd:\\\\NPL\\\\.venv\\\\nltk_data'\n    - 'd:\\\\NPL\\\\.venv\\\\share\\\\nltk_data'\n    - 'd:\\\\NPL\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\yassi\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Extract key sentences from both books\u001b[39;00m\n\u001b[32m     72\u001b[39m n_per_book = \u001b[32m25\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m PetSemetary_sentences = \u001b[43mextract_key_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPetSemetary_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_per_book\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m TheShiningsentences = extract_key_sentences(TheShining_text, n_per_book)\n\u001b[32m     76\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExtracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(PetSemetary_sentences)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m key sentences from PetSemetary\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mextract_key_sentences\u001b[39m\u001b[34m(text, n_sentences)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mExtract key sentences using a simple extractive approach:\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m- Filter by length (not too short, not too long)\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m- Prefer sentences with dialogue or interesting content\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m- Avoid repetitive patterns\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Split into sentences\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m sentences = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Clean and filter sentences\u001b[39;00m\n\u001b[32m     13\u001b[39m filtered = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NPL\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NPL\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NPL\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NPL\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NPL\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\yassi/nltk_data'\n    - 'd:\\\\NPL\\\\.venv\\\\nltk_data'\n    - 'd:\\\\NPL\\\\.venv\\\\share\\\\nltk_data'\n    - 'd:\\\\NPL\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\yassi\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Extract meaningful sentences from both books\n",
    "def extract_key_sentences(text, n_sentences=30):\n",
    "    \"\"\"\n",
    "    Extract key sentences using a simple extractive approach:\n",
    "    - Filter by length (not too short, not too long)\n",
    "    - Prefer sentences with dialogue or interesting content\n",
    "    - Avoid repetitive patterns\n",
    "    \"\"\"\n",
    "    # Split into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # Clean and filter sentences\n",
    "    filtered = []\n",
    "    for sent in sentences:\n",
    "        # Remove extra whitespace\n",
    "        sent = ' '.join(sent.split())\n",
    "        \n",
    "        # Filter criteria\n",
    "        word_count = len(sent.split())\n",
    "        if word_count < 8 or word_count > 40:  # Not too short or long\n",
    "            continue\n",
    "        if sent.startswith('CHAPTER'):  # Skip chapter headers\n",
    "            continue\n",
    "        if re.match(r'^[IVX]+\\.', sent):  # Skip Roman numerals\n",
    "            continue\n",
    "        \n",
    "        filtered.append(sent)\n",
    "    \n",
    "    # Score sentences based on interesting features\n",
    "    scores = []\n",
    "    for sent in filtered:\n",
    "        score = 0\n",
    "        \n",
    "        # Dialogue is interesting\n",
    "        if '\"' in sent or '\"' in sent or '\"' in sent:\n",
    "            score += 2\n",
    "        \n",
    "        # Questions are interesting\n",
    "        if '?' in sent:\n",
    "            score += 1\n",
    "        \n",
    "        # Exclamations add emotion\n",
    "        if '!' in sent:\n",
    "            score += 1\n",
    "        \n",
    "        # Prefer sentences with PetSemetary\n",
    "        if 'PetSemetarymetary' in sent:\n",
    "            score += 1\n",
    "        \n",
    "        # Length sweet spot\n",
    "        word_count = len(sent.split())\n",
    "        if 12 <= word_count <= 25:\n",
    "            score += 1\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    # Get top N sentences by score\n",
    "    scored_sentences = list(zip(filtered, scores))\n",
    "    scored_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top sentences (without duplicates)\n",
    "    top_sentences = []\n",
    "    for sent, score in scored_sentences[:n_sentences * 2]:\n",
    "        if sent not in top_sentences:\n",
    "            top_sentences.append(sent)\n",
    "        if len(top_sentences) >= n_sentences:\n",
    "            break\n",
    "    \n",
    "    return top_sentences\n",
    "\n",
    "# Extract key sentences from both books\n",
    "n_per_book = 25\n",
    "PetSemetary_sentences = extract_key_sentences(PetSemetary_text, n_per_book)\n",
    "TheShiningsentences = extract_key_sentences(TheShining_text, n_per_book)\n",
    "\n",
    "print(f\"Extracted {len(PetSemetary_sentences)} key sentences from PetSemetary\")\n",
    "print(f\"Extracted {len(TheShiningsentences)} key sentences from TheShining\")\n",
    "print(f\"\\nTotal sentences to analyze: {len(PetSemetary_sentences) + len(TheShiningsentences)}\")\n",
    "\n",
    "# Show a few examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE FROM PetSemetaryy:\")\n",
    "print(\"=\"*80)\n",
    "for i, sent in enumerate(PetSemetary_sentences[:3], 1):\n",
    "    print(f\"{i}. {sent}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE FROM TheShining:\")\n",
    "print(\"=\"*80)\n",
    "for i, sent in enumerate(TheShiningsentences[:3], 1):\n",
    "    print(f\"{i}. {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence transformer model and generate embeddings\n",
    "print(\"Loading sentence transformer model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Combine sentences and track their sources\n",
    "all_sentences = PetSemetary_sentences + TheShiningsentences\n",
    "book_labels = ['PetSemetary'] * len(PetSemetary_sentences) + ['TheShining'] * len(TheShiningsentences)\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = model.encode(all_sentences, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nEmbedding shape: {embeddings.shape}\")\n",
    "print(f\"Each sentence → {embeddings.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering to identify themes\n",
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "print(f\"Identified {n_clusters} thematic clusters\")\n",
    "print(\"\\nCluster distribution:\")\n",
    "for i in range(n_clusters):\n",
    "    count = sum(clusters == i)\n",
    "    PetSemetary_count = sum((clusters == i) & (np.array(book_labels) == 'PetSemetary'))\n",
    "    TheShining_count = sum((clusters == i) & (np.array(book_labels) == 'TheShining'))\n",
    "    print(f\"  Cluster {i}: {count} sentences (W:{PetSemetary_count}, LG:{TheShining_count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9217825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Define colors for the two books\n",
    "colors_dict = {'PetSemetary': '#e74c3c', 'TheShining': '#3498db'}\n",
    "colors = [colors_dict[label] for label in book_labels]\n",
    "\n",
    "# --- Plot 1: 2D PCA with book distinction ---\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "pca_2d = PCA(n_components=2)\n",
    "points_2d = pca_2d.fit_transform(embeddings)\n",
    "\n",
    "for book in ['PetSemetary', 'TheShining']:\n",
    "    mask = np.array(book_labels) == book\n",
    "    ax1.scatter(points_2d[mask, 0], points_2d[mask, 1], \n",
    "               c=colors_dict[book], label=book, alpha=0.6, s=100, edgecolors='black', linewidth=1)\n",
    "\n",
    "ax1.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)\n",
    "ax1.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)\n",
    "ax1.set_title('Semantic Space: Two Books Compared\\n(2D PCA projection)', fontsize=12, fontweight='bold')\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 2: 2D PCA with clusters ---\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "scatter = ax2.scatter(points_2d[:, 0], points_2d[:, 1], c=clusters, \n",
    "                     cmap='tab10', alpha=0.6, s=100, edgecolors='black', linewidth=1)\n",
    "ax2.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})', fontsize=11)\n",
    "ax2.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})', fontsize=11)\n",
    "ax2.set_title(f'Thematic Clusters ({n_clusters} themes)\\n(K-Means clustering)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "cbar = plt.colorbar(scatter, ax=ax2)\n",
    "cbar.set_label('Cluster ID', rotation=270, labelpad=15)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 3: 3D PCA ---\n",
    "ax3 = plt.subplot(2, 3, 3, projection='3d')\n",
    "pca_3d = PCA(n_components=3)\n",
    "points_3d = pca_3d.fit_transform(embeddings)\n",
    "\n",
    "for book in ['PetSemetary', 'TheShining']:\n",
    "    mask = np.array(book_labels) == book\n",
    "    ax3.scatter(points_3d[mask, 0], points_3d[mask, 1], points_3d[mask, 2],\n",
    "               c=colors_dict[book], label=book, alpha=0.6, s=60, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "ax3.set_xlabel(f'PC1 ({pca_3d.explained_variance_ratio_[0]:.1%})', fontsize=9)\n",
    "ax3.set_ylabel(f'PC2 ({pca_3d.explained_variance_ratio_[1]:.1%})', fontsize=9)\n",
    "ax3.set_zlabel(f'PC3 ({pca_3d.explained_variance_ratio_[2]:.1%})', fontsize=9)\n",
    "ax3.set_title('3D Semantic Space', fontsize=12, fontweight='bold')\n",
    "ax3.legend(loc='best', fontsize=9)\n",
    "\n",
    "# --- Plot 4: Cross-book similarity matrix ---\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "\n",
    "# Compute similarity between PetSemetary and TheShining\n",
    "PetSemetary_embeddings = embeddings[:len(PetSemetary_sentences)]\n",
    "TheShiningbeddings = embeddings[len(PetSemetary_sentences):]\n",
    "cross_similarity = util.cos_sim(PetSemetary_embeddings, TheShiningbeddings).numpy()\n",
    "\n",
    "im = ax4.imshow(cross_similarity, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n",
    "cbar = plt.colorbar(im, ax=ax4)\n",
    "cbar.set_label('Cosine Similarity', rotation=270, labelpad=15, fontsize=10)\n",
    "ax4.set_xlabel('TheShining Sentences', fontsize=11)\n",
    "ax4.set_ylabel('PetSemetary Sentences', fontsize=11)\n",
    "ax4.set_title('Cross-Book Semantic Similarity\\n(PetSemetary vs TheShining)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "\n",
    "# --- Plot 5: Average similarity per cluster ---\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "\n",
    "cluster_similarities = []\n",
    "for i in range(n_clusters):\n",
    "    cluster_mask = clusters == i\n",
    "    if sum(cluster_mask) > 1:\n",
    "        cluster_emb = embeddings[cluster_mask]\n",
    "        sim_matrix = util.cos_sim(cluster_emb, cluster_emb).numpy()\n",
    "        # Average similarity (excluding diagonal)\n",
    "        avg_sim = (sim_matrix.sum() - sim_matrix.trace()) / (sim_matrix.size - len(cluster_emb))\n",
    "        cluster_similarities.append(avg_sim)\n",
    "    else:\n",
    "        cluster_similarities.append(0)\n",
    "\n",
    "bars = ax5.bar(range(n_clusters), cluster_similarities, color='steelblue', edgecolor='black', linewidth=1.5)\n",
    "ax5.set_xlabel('Cluster ID', fontsize=11)\n",
    "ax5.set_ylabel('Avg. Intra-cluster Similarity', fontsize=11)\n",
    "ax5.set_title('Thematic Coherence by Cluster\\n(Higher = more semantically unified)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax5.set_xticks(range(n_clusters))\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# --- Plot 6: Distribution histogram ---\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "\n",
    "PetSemetary_pc1 = points_2d[:len(PetSemetary_sentences), 0]\n",
    "TheShining_pc1 = points_2d[len(TheShiningsentences):, 0]\n",
    "\n",
    "ax6.hist(PetSemetary_pc1, bins=15, alpha=0.6, color='#e74c3c', label='PetSemetary', edgecolor='black')\n",
    "ax6.hist(TheShining_pc1, bins=15, alpha=0.6, color='#3498db', label='TheShining', edgecolor='black')\n",
    "ax6.set_xlabel('PC1 Value', fontsize=11)\n",
    "ax6.set_ylabel('Frequency', fontsize=11)\n",
    "ax6.set_title('Distribution Along Primary Component\\n(Captures largest semantic variance)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax6.legend(loc='best')\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e9595",
   "metadata": {},
   "source": [
    "### Interpreting the Visualizations\n",
    "\n",
    "**Top Left - Book Comparison (2D):**\n",
    "- Red points = PetSemetary\n",
    "- Blue points = TheShining\n",
    "- Spatial proximity indicates semantic similarity\n",
    "- Overlapping regions suggest shared themes between books\n",
    "\n",
    "**Top Middle - Thematic Clusters:**\n",
    "- Color-coded by automatically detected themes\n",
    "- K-Means clustering groups semantically similar sentences\n",
    "- Reveals common narrative patterns across both books\n",
    "\n",
    "**Top Right - 3D Semantic Space:**\n",
    "- Three-dimensional view captures more variance\n",
    "- Rotation would show different perspectives on semantic relationships\n",
    "- Preserves more information than 2D projection\n",
    "\n",
    "**Bottom Left - Cross-Book Similarity Matrix:**\n",
    "- Heat map showing similarity between every PetSemetary sentence and every TheShining sentence\n",
    "- Bright areas indicate highly similar passages\n",
    "- Reveals which scenes/themes are shared across books\n",
    "\n",
    "**Bottom Middle - Cluster Coherence:**\n",
    "- Measures how semantically unified each theme cluster is\n",
    "- Higher bars = sentences in that cluster are more similar to each other\n",
    "- Indicates strength of thematic grouping\n",
    "\n",
    "**Bottom Right - Distribution Analysis:**\n",
    "- Shows how the books differ along the primary semantic dimension\n",
    "- Overlapping distributions suggest similar semantic content\n",
    "- Separated peaks would indicate distinct narrative styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f27a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each cluster to understand themes\n",
    "print(\"=\"*80)\n",
    "print(\"THEMATIC ANALYSIS OF CLUSTERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_mask = clusters == cluster_id\n",
    "    cluster_sentences = [sent for sent, mask in zip(all_sentences, cluster_mask) if mask]\n",
    "    cluster_books = [book for book, mask in zip(book_labels, cluster_mask) if mask]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CLUSTER {cluster_id}: {len(cluster_sentences)} sentences\")\n",
    "    print(f\"  PetSemetary: {cluster_books.count('PetSemetary')}, TheShining: {cluster_books.count('TheShining')}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Show representative sentences (up to 3)\n",
    "    for i, (sent, book) in enumerate(zip(cluster_sentences[:3], cluster_books[:3]), 1):\n",
    "        book_abbr = \"W\" if book == \"PetSemetary\" else \"LG\"\n",
    "        # Truncate if too long\n",
    "        display_sent = sent if len(sent) < 100 else sent[:97] + \"...\"\n",
    "        print(f\"{i}. [{book_abbr}] {display_sent}\")\n",
    "    \n",
    "    if len(cluster_sentences) > 3:\n",
    "        print(f\"   ... and {len(cluster_sentences) - 3} more sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d16d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most similar cross-book sentence pairs\n",
    "print(\"=\"*80)\n",
    "print(\"MOST SIMILAR SENTENCES ACROSS BOOKS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Finding thematic parallels between PetSemetary and TheShining...\\n\")\n",
    "\n",
    "# Get cross-book similarities\n",
    "PetSemetary_embeddings = embeddings[:len(PetSemetary_sentences)]\n",
    "TheShining_embeddings = embeddings[len(PetSemetary_sentences):]\n",
    "cross_similarity = util.cos_sim(PetSemetary_embeddings, TheShining_embeddings).numpy()\n",
    "\n",
    "# Find top 5 most similar pairs\n",
    "pairs = []\n",
    "for i in range(len(PetSemetary_sentences)):\n",
    "    for j in range(len(TheShiningsentences)):\n",
    "        pairs.append((i, j, cross_similarity[i, j]))\n",
    "\n",
    "pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "for rank, (i, j, score) in enumerate(pairs[:5], 1):\n",
    "    print(f\"{rank}. Similarity: {score:.4f}\")\n",
    "    print(f\"   [PetSemetary] {PetSemetary_sentences[i][:120]}...\")\n",
    "    print(f\"   [TheShining] {TheShiningsentences[j][:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze average within-book vs cross-book similarity\n",
    "PetSemetary_sim = util.cos_sim(PetSemetary_embeddings, PetSemetary_embeddings).numpy()\n",
    "TheShining_sim = util.cos_sim(TheShining_embeddings, TheShining_embeddings).numpy()\n",
    "\n",
    "# Calculate averages (excluding diagonal for within-book)\n",
    "def avg_similarity_no_diag(matrix):\n",
    "    return (matrix.sum() - matrix.trace()) / (matrix.size - len(matrix))\n",
    "\n",
    "avg_PetSemetary = avg_similarity_no_diag(PetSemetary_sim)\n",
    "avg_TheShining = avg_similarity_no_diag(TheShining_sim)\n",
    "avg_cross_book = cross_similarity.mean()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARATIVE SIMILARITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Average within-book similarity (PetSemetary):     {avg_PetSemetary:.4f}\")\n",
    "print(f\"Average within-book similarity (TheShining):  {avg_TheShining:.4f}\")\n",
    "print(f\"Average cross-book similarity:                   {avg_cross_book:.4f}\")\n",
    "print()\n",
    "\n",
    "if avg_cross_book > min(avg_PetSemetary, avg_TheShining) * 0.9:\n",
    "    print(\"✓ The books show HIGH thematic overlap - similar narrative patterns and themes\")\n",
    "elif avg_cross_book > min(avg_PetSemetary, avg_TheShining) * 0.7:\n",
    "    print(\"≈ The books show MODERATE thematic overlap - some shared themes but distinct stories\")\n",
    "else:\n",
    "    print(\"✗ The books show LOW thematic overlap - quite different in content and themes\")\n",
    "\n",
    "print(\"\\nThis suggests that Lewis Carroll maintained\", end=\" \")\n",
    "if avg_cross_book > 0.5:\n",
    "    print(\"consistent themes and writing style across both PetSemetary books.\")\n",
    "else:\n",
    "    print(\"distinct approaches between the two PetSemetary narratives.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c335683",
   "metadata": {},
   "source": [
    "### Key Insights from This Analysis\n",
    "\n",
    "**What We Discovered:**\n",
    "1. **Semantic Embeddings** transform text into numbers while preserving meaning\n",
    "2. **Dimensionality Reduction** (PCA) reveals hidden patterns in high-dimensional data\n",
    "3. **Clustering** automatically discovers thematic groups without manual labeling\n",
    "4. **Cross-book Analysis** quantifies narrative similarities between related texts\n",
    "\n",
    "**Literary Applications:**\n",
    "- **Authorship Analysis**: Detect writing style consistency across works\n",
    "- **Theme Tracking**: Identify recurring motifs and narrative patterns\n",
    "- **Comparative Literature**: Quantify similarities between different texts\n",
    "- **Character Development**: Track how character representations evolve\n",
    "- **Influence Detection**: Measure semantic similarity between works from different authors\n",
    "\n",
    "**Limitations to Consider:**\n",
    "- PCA dimensionality reduction loses information (check variance percentages)\n",
    "- Sentence extraction is simplified - more sophisticated methods exist\n",
    "- Context beyond sentence boundaries is not captured\n",
    "- Model was trained on modern text, may not fully capture Victorian nuances\n",
    "- Clustering number (k=5) was arbitrary - could be optimized\n",
    "\n",
    "**Extensions You Could Try:**\n",
    "- Analyze chapter-by-chapter semantic evolution\n",
    "- Compare PetSemetary books to other Victorian literature\n",
    "- Track character-specific language patterns\n",
    "- Identify dialogue vs. narrative differences\n",
    "- Build a semantic search engine for the texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
